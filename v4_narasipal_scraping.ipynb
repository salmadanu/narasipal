{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V4 Narasipal Scraping\n",
    "- Revised scripts for Kompas and CNN Indonesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from seleniumbase import Driver\n",
    "import csv\n",
    "import time\n",
    "import sys\n",
    "sys.path.append(\"/Users/salmadanu/Desktop/Skripsi/skripsi-env/skripsienv/lib/python3.9/site-packages\")\n",
    "import undetected_chromedriver as uc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [DONE] Republika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_driver_republika(linknum):\n",
    "    driver = Driver(uc=True)\n",
    "    driver.get(f\"https://republika.co.id/tag/palestina/{linknum}\")\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_articles_from_page_republika(driver):\n",
    "    time.sleep(5)\n",
    "    my_page = driver.page_source\n",
    "    my_html = BeautifulSoup(my_page, \"html.parser\")\n",
    "    articles = my_html.find_all('li', class_='list-group-item list-border conten1')\n",
    "    \n",
    "    scraped_data = []\n",
    "    for article in articles:\n",
    "        link_tag = article.find('a', href=True)\n",
    "        url = link_tag['href'] if link_tag else None\n",
    "        \n",
    "        date_span = article.find('div', class_='date')\n",
    "        date_time = date_span.text.split(' - ')[-1] if date_span else None\n",
    "        \n",
    "        title_tag = article.find('h3').find('span') if article.find('h3') else None\n",
    "        title = title_tag.text.strip() if title_tag else None\n",
    "        \n",
    "        if url and date_time and title:\n",
    "            scraped_data.append([url, date_time, title])\n",
    "    \n",
    "    return scraped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  \n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "driver = uc.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = []\n",
    "try:\n",
    "    for link_num in range(60, 90, 15):  # Halaman terakhir ga include, tambahin step 15\n",
    "        print(f\"Scraping links from page {link_num}...\")\n",
    "        driver.get(f\"https://republika.co.id/tag/palestina/{link_num}\")\n",
    "        page_articles = scrape_articles_from_page_republika(driver)\n",
    "        all_articles.extend(page_articles)\n",
    "except Exception as e:\n",
    "    print(f\"Error while scraping links: {e}\")\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "if all_articles:\n",
    "    output_file = '/Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/republika/republika_5-6.csv'\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"article_link\", \"date_time\", \"article_title\"])\n",
    "        for article in all_articles:\n",
    "            writer.writerow(article)\n",
    "\n",
    "print(f\"Articles written to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine CSV Republika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master CSV saved to /Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/master_csv/republika_master.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"/Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/republika\"\n",
    "csv_files = [os.path.join(file_path, f) for f in os.listdir(file_path) if f.endswith('.csv')]\n",
    "\n",
    "dataframes = []\n",
    "for f in csv_files:\n",
    "    df = pd.read_csv(f)\n",
    "    if 'article_link' in df.columns:\n",
    "        df.rename(columns={'article_link': 'url'}, inplace=True)\n",
    "    dataframes.append(df)\n",
    "\n",
    "republika_master = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "output_path = \"/Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/master_csv/republika_master.csv\"\n",
    "republika_master.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Master CSV saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_excel = \"/Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/master_excel/republika_master.xlsx\"\n",
    "republika_master.to_excel(output_path_excel, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [DONE] Detik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_driver_detik(pagenum):\n",
    "    driver = Driver(uc=True)\n",
    "    driver.get(f\"https://www.detik.com/tag/palestina/?sortby=time&page={pagenum}\")\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_articles_from_page_detik(driver):\n",
    "    time.sleep(5)\n",
    "    my_page = driver.page_source\n",
    "    my_html = BeautifulSoup(my_page, \"html.parser\")\n",
    "    \n",
    "    # Find all articles\n",
    "    articles = my_html.find_all('article')\n",
    "\n",
    "    scraped_data = []\n",
    "    for article in articles:\n",
    "        link_tag = article.find('a', href=True)\n",
    "        url = link_tag['href'] if link_tag else None\n",
    "        \n",
    "        date_span = article.find('span', class_='date')\n",
    "        date_time = date_span.text.split(', ')[-1] if date_span else None\n",
    "        \n",
    "        title_tag = article.find('h2', class_='title')\n",
    "        title = title_tag.text.strip() if title_tag else None\n",
    "\n",
    "        if url and date_time and title:\n",
    "            scraped_data.append([url, date_time, title])\n",
    "    \n",
    "    return scraped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  \n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "driver = uc.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = []\n",
    "try:\n",
    "    for page_num in range(105, 125):  # Halaman terakhir ga include\n",
    "        print(f\"Scraping links from page {page_num}...\")\n",
    "        driver.get(f\"https://www.detik.com/tag/palestina/?sortby=time&page={page_num}\")\n",
    "        page_articles = scrape_articles_from_page_detik(driver)\n",
    "        all_articles.extend(page_articles)\n",
    "except Exception as e:\n",
    "    print(f\"Error while scraping links: {e}\")\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "if all_articles:\n",
    "    output_file = '/Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/detik/detik_105-124.csv'\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"article_link\", \"date_time\", \"article_title\"])\n",
    "        for article in all_articles:\n",
    "            writer.writerow(article)\n",
    "\n",
    "print(f\"Articles written to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine CSV Detik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master CSV saved to /Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/master_csv/detik_master.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"/Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/detik\"\n",
    "csv_files = [os.path.join(file_path, f) for f in os.listdir(file_path) if f.endswith('.csv')]\n",
    "\n",
    "dataframes = []\n",
    "for f in csv_files:\n",
    "    df = pd.read_csv(f)\n",
    "    if 'article_link' in df.columns:\n",
    "        df.rename(columns={'article_link': 'url'}, inplace=True)\n",
    "    dataframes.append(df)\n",
    "\n",
    "detik_master = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "output_path = \"/Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/master_csv/detik_master.csv\"\n",
    "detik_master.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Master CSV saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_excel = \"/Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/master_excel/detik_master.xlsx\"\n",
    "detik_master.to_excel(output_path_excel, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [DONE] Kompas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_driver_kompas(pagenum):\n",
    "    driver = Driver(uc=True)\n",
    "    driver.get(f\"https://www.kompas.com/tag/palestina?page={pagenum}\")\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for scraping bottom 15 articles\n",
    "def scrape_articles_from_page_kompas_bawah(driver):\n",
    "    time.sleep(5)\n",
    "    my_page = driver.page_source\n",
    "    my_html = BeautifulSoup(my_page, \"html.parser\")\n",
    "\n",
    "    article_list = my_html.find('div', class_='latest ga--latest mt2 clearfix -newlayout')  # Get wrapper\n",
    "    if not article_list:\n",
    "        return []\n",
    "\n",
    "    articles = article_list.find_all('div', class_='article__list__title')\n",
    "\n",
    "    scraped_data = []\n",
    "    for article in articles:\n",
    "        link_tag = article.find('a', class_='article__link', href=True)\n",
    "        url = link_tag['href'] if link_tag else None\n",
    "\n",
    "        title = link_tag.text.strip() if link_tag else None\n",
    "\n",
    "        article_info = article.find_parent('div', class_='article__list')\n",
    "        date_tag = article_info.find('div', class_='article__date') if article_info else None\n",
    "        date_time = date_tag.text.strip() if date_tag else None\n",
    "\n",
    "        if url and date_time and title:\n",
    "            scraped_data.append([url, date_time, title])\n",
    "\n",
    "    return scraped_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for scraping top 5 articles\n",
    "def scrape_articles_from_page_kompas_atas(driver, column_class):\n",
    "    time.sleep(5)\n",
    "    my_page = driver.page_source\n",
    "    my_html = BeautifulSoup(my_page, \"html.parser\")\n",
    "\n",
    "    articles = my_html.find_all('div', class_=column_class)\n",
    "\n",
    "    scraped_data = []\n",
    "    for article in articles:\n",
    "        link_tag = article.find('a', class_='article__link', href=True)\n",
    "        url = link_tag['href'] if link_tag else None\n",
    "\n",
    "        date_tag = article.find('div', class_='article__date')\n",
    "        date_time = date_tag.text.strip() if date_tag else None\n",
    "\n",
    "        title_tag = article.find('a', class_='article__link')\n",
    "        title = title_tag.text.strip() if title_tag else None\n",
    "\n",
    "        if url and date_time and title:\n",
    "            scraped_data.append([url, date_time, title])\n",
    "            \n",
    "    return scraped_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  \n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "driver = uc.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = []\n",
    "\n",
    "try:\n",
    "    for page_num in range (1, 20): # Halaman terakhir ga include\n",
    "        print(f\"Scraping links from page {page_num}...\")\n",
    "        driver.get(f\"https://www.kompas.com/tag/palestina?page={page_num}\")\n",
    "\n",
    "        page_articles_atas_6 = scrape_articles_from_page_kompas_atas(driver, 'col-bs9-6')\n",
    "        page_articles_atas_3 = scrape_articles_from_page_kompas_atas(driver, 'col-bs9-3')\n",
    "\n",
    "        all_articles.extend(page_articles_atas_6)\n",
    "        all_articles.extend(page_articles_atas_3)\n",
    "\n",
    "        page_articles_bawah = scrape_articles_from_page_kompas_bawah(driver)\n",
    "        all_articles.extend(page_articles_bawah)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error while scraping links: {e}\")\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "if all_articles:\n",
    "    output_file = '/Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/kompas/kompas_1-19.csv'\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"article_link\", \"date_time\", \"article_title\"])\n",
    "        for article in all_articles:\n",
    "            writer.writerow(article)\n",
    "\n",
    "print(f\"Articles written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system('say \"Execution Finished\"')  # macOS\n",
    "print('\\a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine CSV Kompas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"/Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/kompas\"\n",
    "csv_files = [os.path.join(file_path, f) for f in os.listdir(file_path) if f.endswith('.csv')]\n",
    "\n",
    "dataframes = []\n",
    "for f in csv_files:\n",
    "    df = pd.read_csv(f)\n",
    "    if 'article_link' in df.columns:\n",
    "        df.rename(columns={'article_link': 'url'}, inplace=True)\n",
    "    dataframes.append(df)\n",
    "\n",
    "kompas_master = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# output_path = \"/Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/master_csv/kompas_master.csv\"\n",
    "# kompas_master.to_csv(output_path, index=False)\n",
    "\n",
    "# print(f\"Master CSV saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_excel = \"/Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/master_excel/kompas_master.xlsx\"\n",
    "kompas_master.to_excel(output_path_excel, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [DONE] CNN Indonesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_driver_cnnindonesia(pagenum):\n",
    "    driver = Driver(uc=True)\n",
    "    driver.get(f\"https://www.cnnindonesia.com/tag/palestina?page={pagenum}\")\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import Comment\n",
    "\n",
    "def scrape_articles_from_page_cnnindonesia(driver):\n",
    "    time.sleep(5)\n",
    "    my_page = driver.page_source\n",
    "    my_html = BeautifulSoup(my_page, \"html.parser\")\n",
    "    articles = my_html.find_all('article', class_='flex-grow')\n",
    "\n",
    "    scraped_data = []\n",
    "    for article in articles:\n",
    "        link_tag = article.find('a', href=True)\n",
    "        url = link_tag['href'] if link_tag else None\n",
    "\n",
    "        # Extract date-time from the comment inside <span class=\"text-xs text-cnn_black_light3\">\n",
    "        date_span = article.find('span', class_='text-xs text-cnn_black_light3')\n",
    "        date_time = None\n",
    "        if date_span:\n",
    "            comment = date_span.find(string=lambda text: isinstance(text, Comment))\n",
    "            if comment:\n",
    "                date_time = comment.strip()\n",
    "\n",
    "        title_tag = article.find('h2', class_='text-cnn_black_light')\n",
    "        title = title_tag.text.strip() if title_tag else None\n",
    "\n",
    "        if url and date_time and title:\n",
    "            scraped_data.append([url, date_time, title])\n",
    "\n",
    "    return scraped_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  \n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "driver = uc.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = []\n",
    "try:\n",
    "    for page_num in range(1, 50):  # Halaman terakhir ga include\n",
    "        print(f\"Scraping links from page {page_num}...\")\n",
    "        driver.get(f\"https://www.cnnindonesia.com/tag/palestina?page={page_num}\")\n",
    "        page_articles = scrape_articles_from_page_cnnindonesia(driver)\n",
    "        all_articles.extend(page_articles)\n",
    "except Exception as e:\n",
    "    print(f\"Error while scraping links: {e}\")\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "if all_articles:\n",
    "    output_file = '/Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/cnnindonesia/cnn_1-49.csv'\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"article_link\", \"date_time\", \"article_title\"])\n",
    "        for article in all_articles:\n",
    "            writer.writerow(article)\n",
    "\n",
    "print(f\"Articles written to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine CNN CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"/Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/cnnindonesia\"\n",
    "csv_files = [os.path.join(file_path, f) for f in os.listdir(file_path) if f.endswith('.csv')]\n",
    "\n",
    "dataframes = []\n",
    "for f in csv_files:\n",
    "    df = pd.read_csv(f)\n",
    "    if 'article_link' in df.columns:\n",
    "        df.rename(columns={'article_link': 'url'}, inplace=True)\n",
    "    dataframes.append(df)\n",
    "\n",
    "cnn_master = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# output_path = \"/Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/master_csv/cnn_master.csv\"\n",
    "# cnn_master.to_csv(output_path, index=False)\n",
    "\n",
    "# print(f\"Master CSV saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_excel = \"/Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/master_excel/cnn_master.xlsx\"\n",
    "cnn_master.to_excel(output_path_excel, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [DONE] CNBC Indonesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_driver_cnbcindonesia(pagenum):\n",
    "    driver = Driver(uc=True)\n",
    "    driver.get(f\"https://www.cnbcindonesia.com/tag/palestina?page={pagenum}\")\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_page_load(driver, timeout=10):\n",
    "    \"\"\"Ensure JavaScript-rendered content fully loads.\"\"\"\n",
    "    WebDriverWait(driver, timeout).until(lambda d: d.execute_script(\"return document.readyState\") == \"complete\")\n",
    "\n",
    "def scroll_until_no_new_content(driver, max_scrolls=10, wait_time=5):\n",
    "    \"\"\"Scroll until no new articles load, or max scrolls reached.\"\"\"\n",
    "    previous_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    for _ in range(max_scrolls):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(wait_time)  # Allow time for new content to load\n",
    "\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == previous_height:  # No new content loaded\n",
    "            break\n",
    "        previous_height = new_height\n",
    "\n",
    "def scrape_articles_from_page_cnbcindonesia(driver):\n",
    "    \"\"\"Scrape article titles and URLs from CNBC Indonesia search results.\"\"\"\n",
    "    try:\n",
    "        wait_for_page_load(driver)\n",
    "        scroll_until_no_new_content(driver)  # Ensure all articles are loaded\n",
    "\n",
    "        my_page = driver.page_source\n",
    "        my_html = BeautifulSoup(my_page, \"html.parser\")\n",
    "\n",
    "        container = my_html.find('div', class_='flex flex-col gap-6')\n",
    "        if not container:\n",
    "            print(\"Container not found.\")\n",
    "            return []\n",
    "\n",
    "        articles = container.find_all('article')\n",
    "        if not articles:\n",
    "            print(\"No articles found on page.\")\n",
    "            return []\n",
    "\n",
    "        scraped_data = []\n",
    "        base_url = \"https://www.cnbcindonesia.com\"\n",
    "\n",
    "        for article in articles:\n",
    "            link_tag = article.find('a', href=True)\n",
    "            url = link_tag['href'] if link_tag else None\n",
    "            if url and not url.startswith(\"http\"):\n",
    "                url = base_url + url  # Ensure full URL\n",
    "\n",
    "            title_tag = article.find('h2', class_='font-semibold text-23 group-hover:text-cnbc-primary-blue')\n",
    "            title = title_tag.text.strip() if title_tag else None\n",
    "\n",
    "            if url and title:\n",
    "                scraped_data.append([url, title])\n",
    "\n",
    "        return scraped_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while scraping page: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  \n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "driver = uc.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = []\n",
    "try:\n",
    "    for page_num in range(12, 50):  # Last page not included\n",
    "        print(f\"Scraping links from page {page_num}...\")\n",
    "        driver.get(f\"https://www.cnbcindonesia.com/tag/palestina?page={page_num}\")\n",
    "        wait_for_page_load(driver)\n",
    "        scroll_until_no_new_content(driver)  # Ensure all articles are loaded\n",
    "        page_articles = scrape_articles_from_page_cnbcindonesia(driver)\n",
    "        all_articles.extend(page_articles)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error while scraping links: {e}\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "# Save results to CSV\n",
    "output_file = '/Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/cnbcindonesia/cnbc_12-49.csv'\n",
    "if all_articles:\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"article_link\", \"article_title\"])\n",
    "        writer.writerows(all_articles)\n",
    "\n",
    "print(f\"Articles written to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine CNBC CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"/Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/cnbcindonesia\"\n",
    "csv_files = [os.path.join(file_path, f) for f in os.listdir(file_path) if f.endswith('.csv')]\n",
    "\n",
    "dataframes = []\n",
    "for f in csv_files:\n",
    "    df = pd.read_csv(f)\n",
    "    if 'article_link' in df.columns:\n",
    "        df.rename(columns={'article_link': 'url'}, inplace=True)\n",
    "    dataframes.append(df)\n",
    "\n",
    "cnbc_master = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# output_path = \"/Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/master_csv/cnbc_master.csv\"\n",
    "# cnbc_master.to_csv(output_path, index=False)\n",
    "\n",
    "# print(f\"Master CSV saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_excel = \"/Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/master_excel/cnbc_master.xlsx\"\n",
    "cnbc_master.to_excel(output_path_excel, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [DONE] Tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import undetected_chromedriver as uc\n",
    "import csv\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Initialize driver\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "driver = uc.Chrome(options=options)\n",
    "\n",
    "def scrape_articles_from_page_tempo(driver, num_pages=276):\n",
    "    all_scraped_data = []\n",
    "\n",
    "    for page in range(num_pages):\n",
    "        print(f\"Scraping page {page+1}...\")\n",
    "\n",
    "        # Wait until articles are loaded\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//figure\"))\n",
    "        )\n",
    "\n",
    "        # Get page source\n",
    "        my_page = driver.page_source\n",
    "        my_html = BeautifulSoup(my_page, \"html.parser\")\n",
    "\n",
    "        # Find articles container\n",
    "        container = my_html.find('div', class_='flex flex-col divide-y divide-neutral-500')\n",
    "        if not container:\n",
    "            print(\"Container not found. Printing page source for debugging...\")\n",
    "            print(my_page[:2000])\n",
    "            return all_scraped_data\n",
    "\n",
    "        # Extract articles\n",
    "        articles = container.find_all('figure')\n",
    "        for article in articles:\n",
    "            link_tag = article.find('a', href=True)\n",
    "            url = link_tag['href'] if link_tag else None\n",
    "\n",
    "            title_tag = article.find('figcaption').find('a') if article.find('figcaption') else None\n",
    "            title = title_tag.text.strip() if title_tag else None\n",
    "\n",
    "            if url and title:\n",
    "                all_scraped_data.append([url, title])\n",
    "\n",
    "        # Click \"Next Page\" button\n",
    "        try:\n",
    "            next_button = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//button[@aria-label='Next Page']\"))\n",
    "            )\n",
    "            ActionChains(driver).move_to_element(next_button).click().perform()\n",
    "            time.sleep(5)  # Wait for new page\n",
    "        except Exception as e:\n",
    "            print(\"Next button not found or not clickable:\", e)\n",
    "            break  # Stop if pagination fails\n",
    "\n",
    "    return all_scraped_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = []\n",
    "driver.get(f\"https://www.tempo.co/tag/palestina\")\n",
    "page_articles = scrape_articles_from_page_tempo(driver)\n",
    "all_articles.extend(page_articles)\n",
    "output_file = '/Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/tempo/tempo_1-276.csv'\n",
    "if all_articles:\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"article_link\", \"article_title\"])\n",
    "        writer.writerows(all_articles)\n",
    "\n",
    "print(f\"Articles written to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Tempo CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master CSV saved to /Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/master_csv/tempo_master.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"/Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/tempo\"\n",
    "csv_files = [os.path.join(file_path, f) for f in os.listdir(file_path) if f.endswith('.csv')]\n",
    "\n",
    "dataframes = []\n",
    "for f in csv_files:\n",
    "    df = pd.read_csv(f)\n",
    "    if 'article_link' in df.columns:\n",
    "        df.rename(columns={'article_link': 'url'}, inplace=True)\n",
    "    dataframes.append(df)\n",
    "\n",
    "tempo_master = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "output_path = \"/Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/master_csv/tempo_master.csv\"\n",
    "tempo_master.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Master CSV saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_excel = \"/Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/master_excel/tempo_master.xlsx\"\n",
    "tempo_master.to_excel(output_path_excel, index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
