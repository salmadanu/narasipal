{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V4 Narasipal Scraping\n",
    "- Revised scripts for Kompas and CNN Indonesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from seleniumbase import Driver\n",
    "import csv\n",
    "import time\n",
    "import sys\n",
    "sys.path.append(\"/Users/salmadanu/Desktop/Skripsi/skripsi-env/skripsienv/lib/python3.9/site-packages\")\n",
    "import undetected_chromedriver as uc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Republika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_driver_republika(linknum):\n",
    "    driver = Driver(uc=True)\n",
    "    driver.get(f\"https://republika.co.id/tag/palestina/{linknum}\")\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_articles_from_page_republika(driver):\n",
    "    time.sleep(5)\n",
    "    my_page = driver.page_source\n",
    "    my_html = BeautifulSoup(my_page, \"html.parser\")\n",
    "    articles = my_html.find_all('li', class_='list-group-item list-border conten1')\n",
    "    \n",
    "    scraped_data = []\n",
    "    for article in articles:\n",
    "        link_tag = article.find('a', href=True)\n",
    "        url = link_tag['href'] if link_tag else None\n",
    "        \n",
    "        date_span = article.find('div', class_='date')\n",
    "        date_time = date_span.text.split(' - ')[-1] if date_span else None\n",
    "        \n",
    "        title_tag = article.find('h3').find('span') if article.find('h3') else None\n",
    "        title = title_tag.text.strip() if title_tag else None\n",
    "        \n",
    "        if url and date_time and title:\n",
    "            scraped_data.append([url, date_time, title])\n",
    "    \n",
    "    return scraped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  \n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "driver = uc.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = []\n",
    "try:\n",
    "    for link_num in range(75, 225, 15):  # Halaman terakhir ga include, tambahin step 15\n",
    "        print(f\"Scraping links from page {link_num}...\")\n",
    "        driver.get(f\"https://republika.co.id/tag/palestina/{link_num}\")\n",
    "        page_articles = scrape_articles_from_page_republika(driver)\n",
    "        all_articles.extend(page_articles)\n",
    "except Exception as e:\n",
    "    print(f\"Error while scraping links: {e}\")\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "if all_articles:\n",
    "    output_file = '/Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/republika/republika_6-15.csv'\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"article_link\", \"date_time\", \"article_title\"])\n",
    "        for article in all_articles:\n",
    "            writer.writerow(article)\n",
    "\n",
    "print(f\"Articles written to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_driver_detik(pagenum):\n",
    "    driver = Driver(uc=True)\n",
    "    driver.get(f\"https://www.detik.com/tag/palestina/?sortby=time&page={pagenum}\")\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_articles_from_page_detik(driver):\n",
    "    time.sleep(5)\n",
    "    my_page = driver.page_source\n",
    "    my_html = BeautifulSoup(my_page, \"html.parser\")\n",
    "    \n",
    "    # Find all articles\n",
    "    articles = my_html.find_all('article')\n",
    "\n",
    "    scraped_data = []\n",
    "    for article in articles:\n",
    "        link_tag = article.find('a', href=True)\n",
    "        url = link_tag['href'] if link_tag else None\n",
    "        \n",
    "        date_span = article.find('span', class_='date')\n",
    "        date_time = date_span.text.split(', ')[-1] if date_span else None\n",
    "        \n",
    "        title_tag = article.find('h2', class_='title')\n",
    "        title = title_tag.text.strip() if title_tag else None\n",
    "\n",
    "        if url and date_time and title:\n",
    "            scraped_data.append([url, date_time, title])\n",
    "    \n",
    "    return scraped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  \n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "driver = uc.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = []\n",
    "try:\n",
    "    for page_num in range(105, 125):  # Halaman terakhir ga include\n",
    "        print(f\"Scraping links from page {page_num}...\")\n",
    "        driver.get(f\"https://www.detik.com/tag/palestina/?sortby=time&page={page_num}\")\n",
    "        page_articles = scrape_articles_from_page_detik(driver)\n",
    "        all_articles.extend(page_articles)\n",
    "except Exception as e:\n",
    "    print(f\"Error while scraping links: {e}\")\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "if all_articles:\n",
    "    output_file = '/Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/detik/detik_105-124.csv'\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"article_link\", \"date_time\", \"article_title\"])\n",
    "        for article in all_articles:\n",
    "            writer.writerow(article)\n",
    "\n",
    "print(f\"Articles written to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kompas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_driver_kompas(pagenum):\n",
    "    driver = Driver(uc=True)\n",
    "    driver.get(f\"https://www.kompas.com/tag/palestina?page={pagenum}\")\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for scraping bottom 15 articles\n",
    "def scrape_articles_from_page_kompas_bawah(driver):\n",
    "    time.sleep(5)\n",
    "    my_page = driver.page_source\n",
    "    my_html = BeautifulSoup(my_page, \"html.parser\")\n",
    "\n",
    "    article_list = my_html.find('div', class_='latest ga--latest mt2 clearfix -newlayout')  # Get wrapper\n",
    "    if not article_list:\n",
    "        return []\n",
    "\n",
    "    articles = article_list.find_all('div', class_='article__list__title')\n",
    "\n",
    "    scraped_data = []\n",
    "    for article in articles:\n",
    "        link_tag = article.find('a', class_='article__link', href=True)\n",
    "        url = link_tag['href'] if link_tag else None\n",
    "\n",
    "        title = link_tag.text.strip() if link_tag else None\n",
    "\n",
    "        article_info = article.find_parent('div', class_='article__list')\n",
    "        date_tag = article_info.find('div', class_='article__date') if article_info else None\n",
    "        date_time = date_tag.text.strip() if date_tag else None\n",
    "\n",
    "        if url and date_time and title:\n",
    "            scraped_data.append([url, date_time, title])\n",
    "\n",
    "    return scraped_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for scraping top 5 articles\n",
    "def scrape_articles_from_page_kompas_atas(driver, column_class):\n",
    "    time.sleep(5)\n",
    "    my_page = driver.page_source\n",
    "    my_html = BeautifulSoup(my_page, \"html.parser\")\n",
    "\n",
    "    articles = my_html.find_all('div', class_=column_class)\n",
    "\n",
    "    scraped_data = []\n",
    "    for article in articles:\n",
    "        link_tag = article.find('a', class_='article__link', href=True)\n",
    "        url = link_tag['href'] if link_tag else None\n",
    "\n",
    "        date_tag = article.find('div', class_='article__date')\n",
    "        date_time = date_tag.text.strip() if date_tag else None\n",
    "\n",
    "        title_tag = article.find('a', class_='article__link')\n",
    "        title = title_tag.text.strip() if title_tag else None\n",
    "\n",
    "        if url and date_time and title:\n",
    "            scraped_data.append([url, date_time, title])\n",
    "            \n",
    "    return scraped_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  \n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "driver = uc.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping links from page 310...\n",
      "Articles written to /Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/kompas_new_fix/kompas_310.csv\n"
     ]
    }
   ],
   "source": [
    "all_articles = []\n",
    "\n",
    "try:\n",
    "    for page_num in range (310, 311): # Halaman terakhir ga include\n",
    "        print(f\"Scraping links from page {page_num}...\")\n",
    "        driver.get(f\"https://www.kompas.com/tag/palestina?page={page_num}\")\n",
    "\n",
    "        page_articles_atas_6 = scrape_articles_from_page_kompas_atas(driver, 'col-bs9-6')\n",
    "        page_articles_atas_3 = scrape_articles_from_page_kompas_atas(driver, 'col-bs9-3')\n",
    "\n",
    "        all_articles.extend(page_articles_atas_6)\n",
    "        all_articles.extend(page_articles_atas_3)\n",
    "\n",
    "        page_articles_bawah = scrape_articles_from_page_kompas_bawah(driver)\n",
    "        all_articles.extend(page_articles_bawah)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error while scraping links: {e}\")\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "if all_articles:\n",
    "    output_file = '/Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/kompas_new_fix/kompas_310.csv'\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"article_link\", \"date_time\", \"article_title\"])\n",
    "        for article in all_articles:\n",
    "            writer.writerow(article)\n",
    "\n",
    "print(f\"Articles written to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
