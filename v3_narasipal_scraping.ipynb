{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V3 Narasipal Scraping\n",
    "Links only, no other attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from seleniumbase import Driver\n",
    "import csv\n",
    "import time\n",
    "import sys\n",
    "sys.path.append(\"/Users/salmadanu/Desktop/Skripsi/skripsi-env/skripsienv/lib/python3.9/site-packages\")\n",
    "import undetected_chromedriver as uc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kompas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_driver_kompas(pagenum):\n",
    "    driver = Driver(uc=True)\n",
    "    driver.get(f\"https://www.kompas.com/tag/palestina?page={pagenum}\")\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links_from_page_kompas(driver):\n",
    "    time.sleep(5)\n",
    "    my_page = driver.page_source\n",
    "    my_html = BeautifulSoup(my_page, \"html.parser\")\n",
    "    card_containers = my_html.find_all('div', class_='article__list clearfix')\n",
    "\n",
    "    nav_links = []\n",
    "    for container in card_containers:\n",
    "        link_link = container.find('a', href=True)\n",
    "        href = link_link['href'] if link_link else None\n",
    "        if href:\n",
    "            nav_links.append(href)\n",
    "    return nav_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  \n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "driver = uc.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine CSV Kompas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"/Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/kompas_new\"\n",
    "csv_files = [os.path.join(file_path, f) for f in os.listdir(file_path) if f.endswith('.csv')]\n",
    "\n",
    "dataframes = []\n",
    "for f in csv_files:\n",
    "    df = pd.read_csv(f)\n",
    "    if 'article_link' in df.columns:\n",
    "        df.rename(columns={'article_link': 'url'}, inplace=True)\n",
    "    dataframes.append(df)\n",
    "\n",
    "kompas_master = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "output_path = \"/Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/master_csv/kompas_master.csv\"\n",
    "kompas_master.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Master CSV saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nav_links = []\n",
    "try:\n",
    "    for page_num in range(6, 70): # Halaman terakhir ga include\n",
    "        print(f\"Scraping links from page {page_num}...\")\n",
    "        driver.get(f\"https://www.kompas.com/tag/palestina?page={page_num}\")\n",
    "        page_links = scrape_links_from_page_kompas(driver)\n",
    "        all_nav_links.extend(page_links)\n",
    "except Exception as e:\n",
    "    print(f\"Error while scraping links: {e}\")\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "if all_nav_links:\n",
    "    output_file = '/Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/kompas_new/kompas_6-69.csv'\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"article_link\"])\n",
    "    for link in all_nav_links:\n",
    "        writer.writerow([link])\n",
    "\n",
    "print(f\"Links written to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Republika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_driver_republika(linknum):\n",
    "    driver = Driver(uc=True)\n",
    "    driver.get(f\"https://republika.co.id/tag/palestina/{linknum}\")\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_articles_from_page_republika(driver):\n",
    "    time.sleep(5)\n",
    "    my_page = driver.page_source\n",
    "    my_html = BeautifulSoup(my_page, \"html.parser\")\n",
    "    articles = my_html.find_all('li', class_='list-group-item list-border conten1')\n",
    "    \n",
    "    scraped_data = []\n",
    "    for article in articles:\n",
    "        link_tag = article.find('a', href=True)\n",
    "        url = link_tag['href'] if link_tag else None\n",
    "        \n",
    "        date_span = article.find('div', class_='date')\n",
    "        date_time = date_span.text.split(' - ')[-1] if date_span else None\n",
    "        \n",
    "        title_tag = article.find('h3').find('span') if article.find('h3') else None\n",
    "        title = title_tag.text.strip() if title_tag else None\n",
    "        \n",
    "        if url and date_time and title:\n",
    "            scraped_data.append([url, date_time, title])\n",
    "    \n",
    "    return scraped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  \n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "driver = uc.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = []\n",
    "try:\n",
    "    for link_num in range(75, 225, 15):  # Halaman terakhir ga include, tambahin step 15\n",
    "        print(f\"Scraping links from page {link_num}...\")\n",
    "        driver.get(f\"https://republika.co.id/tag/palestina/{link_num}\")\n",
    "        page_articles = scrape_articles_from_page_republika(driver)\n",
    "        all_articles.extend(page_articles)\n",
    "except Exception as e:\n",
    "    print(f\"Error while scraping links: {e}\")\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "if all_articles:\n",
    "    output_file = '/Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/republika/republika_6-15.csv'\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"article_link\", \"date_time\", \"article_title\"])\n",
    "        for article in all_articles:\n",
    "            writer.writerow(article)\n",
    "\n",
    "print(f\"Articles written to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_driver_cnnindonesia(pagenum):\n",
    "    driver = Driver(uc=True)\n",
    "    driver.get(f\"https://www.cnnindonesia.com/tag/palestina?page={pagenum}\")\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links_from_page_cnnindonesia(driver):\n",
    "    time.sleep(5)\n",
    "    my_page = driver.page_source\n",
    "    my_html = BeautifulSoup(my_page, \"html.parser\")\n",
    "\n",
    "    # Find all div containers with the target class\n",
    "    card_containers = my_html.find_all('div', class_='flex flex-col gap-5')\n",
    "\n",
    "    nav_links = []\n",
    "    for container in card_containers:\n",
    "        links = container.find_all('a', href=True)  # Get all <a> tags inside the container\n",
    "        for link in links:\n",
    "            href = link['href']\n",
    "            if href.startswith(\"https://www.cnnindonesia.com/\"):  # Ensure it's an article link\n",
    "                nav_links.append(href)\n",
    "\n",
    "    return list(set(nav_links))  # Remove duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  \n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "driver = uc.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nav_links = []\n",
    "try:\n",
    "    for page_num in range(18, 50): # Halaman terakhir ga include\n",
    "        print(f\"Scraping links from page {page_num}...\")\n",
    "        driver.get(f\"https://www.cnnindonesia.com/tag/palestina?page={page_num}\")\n",
    "        page_links = scrape_links_from_page_cnnindonesia(driver)\n",
    "        all_nav_links.extend(page_links)\n",
    "except Exception as e:\n",
    "    print(f\"Error while scraping links: {e}\")\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "if all_nav_links:\n",
    "    output_file = '/Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/cnnindonesia/cnn_18-49.csv'\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"article_link\"])\n",
    "    for link in all_nav_links:\n",
    "        writer.writerow([link])\n",
    "\n",
    "print(f\"Links written to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_driver_detik(pagenum):\n",
    "    driver = Driver(uc=True)\n",
    "    driver.get(f\"https://www.detik.com/tag/palestina/?sortby=time&page={pagenum}\")\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_articles_from_page_detik(driver):\n",
    "    time.sleep(5)\n",
    "    my_page = driver.page_source\n",
    "    my_html = BeautifulSoup(my_page, \"html.parser\")\n",
    "    \n",
    "    # Find all articles\n",
    "    articles = my_html.find_all('article')\n",
    "\n",
    "    scraped_data = []\n",
    "    for article in articles:\n",
    "        link_tag = article.find('a', href=True)\n",
    "        url = link_tag['href'] if link_tag else None\n",
    "        \n",
    "        date_span = article.find('span', class_='date')\n",
    "        date_time = date_span.text.split(', ')[-1] if date_span else None\n",
    "        \n",
    "        title_tag = article.find('h2', class_='title')\n",
    "        title = title_tag.text.strip() if title_tag else None\n",
    "\n",
    "        if url and date_time and title:\n",
    "            scraped_data.append([url, date_time, title])\n",
    "    \n",
    "    return scraped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  \n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "driver = uc.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = []\n",
    "try:\n",
    "    for page_num in range(535, 546):  # Halaman terakhir ga include\n",
    "        print(f\"Scraping links from page {page_num}...\")\n",
    "        driver.get(f\"https://www.detik.com/tag/palestina/?sortby=time&page={page_num}\")\n",
    "        page_articles = scrape_articles_from_page_detik(driver)\n",
    "        all_articles.extend(page_articles)\n",
    "except Exception as e:\n",
    "    print(f\"Error while scraping links: {e}\")\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "if all_articles:\n",
    "    output_file = '/Users/salmadanu/Desktop/Skripsi/skripsi-env/narasipal/detik/detik_544-545.csv'\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"article_link\", \"date_time\", \"article_title\"])\n",
    "        for article in all_articles:\n",
    "            writer.writerow(article)\n",
    "\n",
    "print(f\"Articles written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
